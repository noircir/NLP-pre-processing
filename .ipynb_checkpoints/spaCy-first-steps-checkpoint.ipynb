{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import spaCy and create a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# sm = small English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tea is healthy and calming, don't you think?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea\n",
      "is\n",
      "healthy\n",
      "and\n",
      "calming\n",
      ",\n",
      "do\n",
      "n't\n",
      "you\n",
      "think\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmas and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token \t\tLemma \t\tStopword\n",
      "----------------------------------------\n",
      "Tea\t\ttea\t\tFalse\n",
      "is\t\tbe\t\tTrue\n",
      "healthy\t\thealthy\t\tFalse\n",
      "and\t\tand\t\tTrue\n",
      "calming\t\tcalm\t\tFalse\n",
      ",\t\t,\t\tFalse\n",
      "do\t\tdo\t\tTrue\n",
      "n't\t\tnot\t\tTrue\n",
      "you\t\t-PRON-\t\tTrue\n",
      "think\t\tthink\t\tFalse\n",
      "?\t\t?\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(\"{} \\t\\t{} \\t\\t{}\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*40)\n",
    "for token in doc:\n",
    "    print(\"{}\\t\\t{}\\t\\t{}\".format(str(token), token.lemma_, token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The same for French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fr = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = nlp_fr(\"La conscience de n'être sur la terre qu'en sursis, d'une mort qui,\\\n",
    "                quoi qu'il arrive, arrivera, sans espoir de salut. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La\n",
      "conscience\n",
      "de\n",
      "n'\n",
      "être\n",
      "sur\n",
      "la\n",
      "terre\n",
      "qu'\n",
      "en\n",
      "sursis\n",
      ",\n",
      "d'\n",
      "une\n",
      "mort\n",
      "qui\n",
      ",\n",
      "               \n",
      "quoi\n",
      "qu'\n",
      "il\n",
      "arrive\n",
      ",\n",
      "arrivera\n",
      ",\n",
      "sans\n",
      "espoir\n",
      "de\n",
      "salut\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in phrase:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  NB: lemmatizing strange?  (qu' ,   n' ,  arrive  ,  pronoun is not captured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token \t\t\tLemma \t\t\tStopword\n",
      "----------------------------------------------------------------------\n",
      "La\t\t\tle\t\t\tTrue\n",
      "conscience\t\t\tconscience\t\t\tFalse\n",
      "de\t\t\tde\t\t\tTrue\n",
      "n'\t\t\tn'\t\t\tTrue\n",
      "être\t\t\têtre\t\t\tTrue\n",
      "sur\t\t\tsur\t\t\tTrue\n",
      "la\t\t\tle\t\t\tTrue\n",
      "terre\t\t\tterre\t\t\tFalse\n",
      "qu'\t\t\tque\t\t\tTrue\n",
      "en\t\t\ten\t\t\tTrue\n",
      "sursis\t\t\tsursis\t\t\tFalse\n",
      ",\t\t\t,\t\t\tFalse\n",
      "d'\t\t\tde\t\t\tTrue\n",
      "une\t\t\tun\t\t\tTrue\n",
      "mort\t\t\tmort\t\t\tFalse\n",
      "qui\t\t\tqui\t\t\tTrue\n",
      ",\t\t\t,\t\t\tFalse\n",
      "               \t\t\t               \t\t\tFalse\n",
      "quoi\t\t\tquoi\t\t\tTrue\n",
      "qu'\t\t\tqu'\t\t\tTrue\n",
      "il\t\t\til\t\t\tTrue\n",
      "arrive\t\t\tarrive\t\t\tFalse\n",
      ",\t\t\t,\t\t\tFalse\n",
      "arrivera\t\t\tarriver\t\t\tFalse\n",
      ",\t\t\t,\t\t\tFalse\n",
      "sans\t\t\tsans\t\t\tTrue\n",
      "espoir\t\t\tespoir\t\t\tFalse\n",
      "de\t\t\tde\t\t\tTrue\n",
      "salut\t\t\tsalut\t\t\tFalse\n",
      ".\t\t\t.\t\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(\"{} \\t\\t\\t{} \\t\\t\\t{}\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*70)\n",
    "for token in phrase:\n",
    "    print(\"{}\\t\\t\\t{}\\t\\t\\t{}\".format(str(token), token.lemma_, token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common NLP task is matching tokens or phrases within chunks of text or whole documents. You can do pattern matching with regular expressions, but spaCy's matching capabilities tend to be easier to use.\n",
    "\n",
    "To match individual tokens, you create a Matcher. When you want to match a list of terms, it's easier and more efficient to use PhraseMatcher. For example, if you want to find where different smartphone models show up in some text, you can create patterns for the model names of interest. First you create the PhraseMatcher itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# The matcher is created using the vocabulary of the 'small English' model.\n",
    "# Attribute LOWER for case-insensitive matching\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
    "\n",
    "# create a list of terms to match in the text.\n",
    "patterns = [nlp(text) for text in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Galaxy Note, iPhone 11, iPhone XS, Google Pixel]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these patterns to the matcher as \"Terminology List\"\n",
    "\n",
    "# Add a match-rule to the phrase-matcher. \n",
    "# A match-rule consists of: an ID key, an on_match callback, and one or more patterns.\n",
    "matcher.add(\"TerminologyList\", None, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)]\n"
     ]
    }
   ],
   "source": [
    "# Then you create a document from the text to search and use the phrase matcher \n",
    "# to find where the terms occur in the text.\n",
    "\n",
    "# Borrowed from https://daringfireball.net/linked/2019/09/21/patel-11-pro\n",
    "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n",
    "               \"photography tests pitting the iPhone 11 Pro against the \"\n",
    "               \"Galaxy Note 10 Plus and last year’s iPhone XS and Google Pixel 3.\") \n",
    "matches = matcher(text_doc)\n",
    "\n",
    "# The matches here are a tuple of the match id and the positions of the start and end of the phrase.\n",
    "# (vocabulary includes commas and dashes, and count starts at zero)\n",
    "\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TerminologyList iPhone XS\n"
     ]
    }
   ],
   "source": [
    "match_id, start, end = matches[2]\n",
    "print(nlp.vocab.strings[match_id], text_doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
